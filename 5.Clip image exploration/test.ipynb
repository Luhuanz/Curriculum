{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26d575c-94a6-469d-b835-a28c226a42e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\n",
      "Loading vision model config from /root/autodl-tmp/project/cn_clip/clip/model_configs/ViT-B-16.json\n",
      "Loading text model config from /root/autodl-tmp/project/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json\n",
      "Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}\n",
      "Label probs: [[0.002913 0.974    0.01017  0.01266 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from PIL import Image\n",
    "\n",
    "import cn_clip.clip as clip\n",
    "from cn_clip.clip import load_from_name, available_models\n",
    "print(\"Available models:\", available_models())  \n",
    "# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = load_from_name(\"ViT-B-16\", device=device, download_root='./')\n",
    "model.eval()\n",
    "image = preprocess(Image.open(\"imgs/皮卡丘.png\").convert(\"RGBA\")).unsqueeze(0).to(device)  #Label probs: [[0.003006 0.974    0.01017  0.01265 ]]\n",
    "# image = preprocess(Image.open(\"imgs/皮卡丘.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    # 对特征进行归一化，请使用归一化后的图文特征用于下游任务\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True) \n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)    \n",
    "\n",
    "    logits_per_image, logits_per_text = model.get_similarity(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c30ea9-5947-4041-a073-32a34221ec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: epoch, Type: <class 'int'>\n",
      "Layer: step, Type: <class 'int'>\n",
      "Layer: name, Type: <class 'str'>\n",
      "Layer: state_dict, Type: <class 'collections.OrderedDict'>\n",
      "Layer: optimizer, Type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 指定你保存的模型参数的路径\n",
    "saved_model_path = 'epoch_latest.pt'\n",
    "\n",
    "# 加载保存的模型参数\n",
    "model_state_dict = torch.load(saved_model_path)\n",
    "\n",
    "# 打印模型结构\n",
    "for key, value in model_state_dict.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"Layer: {key}, Shape: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"Layer: {key}, Type: {type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebafe39-8d13-423f-9ef3-e90384978b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: module.text_projection, Shape: torch.Size([768, 512])\n",
      "Layer: module.logit_scale, Shape: torch.Size([])\n",
      "Layer: module.visual.class_embedding, Shape: torch.Size([768])\n",
      "Layer: module.visual.positional_embedding, Shape: torch.Size([197, 768])\n",
      "Layer: module.visual.proj, Shape: torch.Size([768, 512])\n",
      "Layer: module.visual.conv1.weight, Shape: torch.Size([768, 3, 16, 16])\n",
      "Layer: module.visual.ln_pre.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.ln_pre.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.0.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.0.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.0.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.0.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.0.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.0.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.0.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.0.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.0.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.0.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.0.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.0.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.1.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.1.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.1.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.1.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.1.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.1.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.1.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.1.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.1.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.1.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.1.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.1.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.2.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.2.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.2.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.2.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.2.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.2.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.2.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.2.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.2.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.2.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.2.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.2.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.3.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.3.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.3.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.3.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.3.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.3.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.3.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.3.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.3.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.3.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.3.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.3.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.4.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.4.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.4.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.4.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.4.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.4.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.4.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.4.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.4.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.4.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.4.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.4.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.5.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.5.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.5.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.5.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.5.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.5.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.5.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.5.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.5.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.5.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.5.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.5.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.6.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.6.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.6.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.6.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.6.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.6.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.6.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.6.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.6.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.6.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.6.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.6.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.7.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.7.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.7.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.7.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.7.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.7.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.7.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.7.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.7.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.7.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.7.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.7.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.8.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.8.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.8.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.8.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.8.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.8.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.8.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.8.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.8.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.8.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.8.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.8.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.9.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.9.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.9.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.9.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.9.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.9.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.9.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.9.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.9.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.9.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.9.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.9.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.10.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.10.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.10.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.10.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.10.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.10.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.10.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.10.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.10.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.10.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.10.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.10.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.11.attn.in_proj_weight, Shape: torch.Size([2304, 768])\n",
      "Layer: module.visual.transformer.resblocks.11.attn.in_proj_bias, Shape: torch.Size([2304])\n",
      "Layer: module.visual.transformer.resblocks.11.attn.out_proj.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.visual.transformer.resblocks.11.attn.out_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.11.ln_1.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.11.ln_1.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.11.mlp.c_fc.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.visual.transformer.resblocks.11.mlp.c_fc.bias, Shape: torch.Size([3072])\n",
      "Layer: module.visual.transformer.resblocks.11.mlp.c_proj.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.visual.transformer.resblocks.11.mlp.c_proj.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.11.ln_2.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.transformer.resblocks.11.ln_2.bias, Shape: torch.Size([768])\n",
      "Layer: module.visual.ln_post.weight, Shape: torch.Size([768])\n",
      "Layer: module.visual.ln_post.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.embeddings.word_embeddings.weight, Shape: torch.Size([21128, 768])\n",
      "Layer: module.bert.embeddings.position_embeddings.weight, Shape: torch.Size([512, 768])\n",
      "Layer: module.bert.embeddings.token_type_embeddings.weight, Shape: torch.Size([2, 768])\n",
      "Layer: module.bert.embeddings.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.embeddings.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.0.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.0.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.0.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.0.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.0.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.0.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.0.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.0.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.1.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.1.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.1.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.1.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.1.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.1.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.1.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.1.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.2.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.2.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.2.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.2.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.2.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.2.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.2.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.2.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.3.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.3.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.3.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.3.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.3.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.3.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.3.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.3.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.4.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.4.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.4.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.4.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.4.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.4.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.4.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.4.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.5.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.5.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.5.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.5.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.5.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.5.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.5.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.5.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.6.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.6.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.6.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.6.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.6.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.6.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.6.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.6.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.7.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.7.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.7.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.7.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.7.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.7.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.7.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.7.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.8.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.8.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.8.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.8.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.8.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.8.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.8.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.8.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.9.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.9.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.9.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.9.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.9.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.9.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.9.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.9.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.10.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.10.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.10.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.10.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.10.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.10.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.10.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.10.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.attention.self.query.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.11.attention.self.query.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.attention.self.key.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.11.attention.self.key.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.attention.self.value.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.11.attention.self.value.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.attention.output.dense.weight, Shape: torch.Size([768, 768])\n",
      "Layer: module.bert.encoder.layer.11.attention.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.attention.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.attention.output.LayerNorm.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.intermediate.dense.weight, Shape: torch.Size([3072, 768])\n",
      "Layer: module.bert.encoder.layer.11.intermediate.dense.bias, Shape: torch.Size([3072])\n",
      "Layer: module.bert.encoder.layer.11.output.dense.weight, Shape: torch.Size([768, 3072])\n",
      "Layer: module.bert.encoder.layer.11.output.dense.bias, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.output.LayerNorm.weight, Shape: torch.Size([768])\n",
      "Layer: module.bert.encoder.layer.11.output.LayerNorm.bias, Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 指定你保存的模型参数的路径\n",
    "saved_model_path = 'epoch_latest.pt'\n",
    "\n",
    "# 加载保存的模型参数\n",
    "checkpoint = torch.load(saved_model_path)\n",
    "\n",
    "# 获取模型权重和偏差\n",
    "model_state_dict = checkpoint['state_dict']\n",
    "\n",
    "# 打印模型结构\n",
    "for key, value in model_state_dict.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"Layer: {key}, Shape: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"Layer: {key}, Type: {type(value)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f9a7c6-b425-4202-bf13-cf8ef76bbb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the state_dict:\n",
      "epoch\n",
      "step\n",
      "name\n",
      "state_dict\n",
      "optimizer\n"
     ]
    }
   ],
   "source": [
    "model_path ='epoch_latest.pt'\n",
    "model_state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# 打印状态字典的键\n",
    "print(\"Keys in the state_dict:\")\n",
    "for key in model_state_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd9c0c7-ff45-4c3e-be02-6f2781813a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision model config from /root/autodl-tmp/project/cn_clip/clip/model_configs/ViT-B-16.json\n",
      "Loading text model config from /root/autodl-tmp/project/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json\n",
      "Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}\n",
      "Label probabilities: [[0.e+00 0.e+00 6.e-08 1.e+00]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cn_clip.clip as clip\n",
    "from PIL import Image\n",
    "\n",
    "# 加载微调后的模型权重\n",
    "model_path = 'epoch_latest.pt'\n",
    "saved_model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model_state_dict = saved_model['state_dict']\n",
    "\n",
    "# 调整状态字典的键\n",
    "adjusted_state_dict = {}\n",
    "for key in model_state_dict.keys():\n",
    "    new_key = key[7:] if key.startswith('module.') else key\n",
    "    adjusted_state_dict[new_key] = model_state_dict[key]\n",
    "\n",
    "# 创建模型实例\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load_from_name(\"ViT-B-16\", device=device)\n",
    "\n",
    "# 加载调整后的状态字典\n",
    "try:\n",
    "    model.load_state_dict(adjusted_state_dict)\n",
    "except RuntimeError as e:\n",
    "    print(\"Error:\", e)\n",
    "    model.load_state_dict(adjusted_state_dict, strict=False)\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 图像预处理\n",
    "image_path = \"imgs/皮卡丘.png\"\n",
    "image = preprocess(Image.open(image_path).convert(\"RGBA\")).unsqueeze(0).to(device)\n",
    "\n",
    "# 文本处理\n",
    "text = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]).to(device)\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    image_features, text_features, logit_scale = model(image, text)\n",
    "\n",
    "    # 归一化特征\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # 计算相似度分数\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "    # 转换为概率\n",
    "    probs_per_image = F.softmax(logits_per_image, dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probabilities:\", probs_per_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a152f99e-8e39-4479-95a4-4351e71b81fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_per_image[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799dd7c-8c28-41e8-bb6d-09e9c10b70f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
