
 

# CLIPå®å¯æ¢¦å›¾åƒæ¢ç´¢

ğŸ‰ ä½¿ç”¨CLIPæ¨¡å‹æ¢ç´¢å®å¯æ¢¦çš„ä¸–ç•Œï¼æœ¬é¡¹ç›®é€šè¿‡å›¾ç‰‡ä¸å¯¹åº”çš„ä¸­æ–‡åå­—ï¼Œä½¿ç”¨å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•æ¥è¯†åˆ«å’Œåˆ†æå®å¯æ¢¦å›¾ç‰‡ã€‚é¡¹ç›®åŸºäºChinese-CLIPæ¨¡å‹ï¼Œä¸“ä¸ºå¤„ç†ä¸­æ–‡æ–‡æœ¬è€Œä¼˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡å›¾ç‰‡ä¸å›¾ç‰‡åè¿›è¡Œå¤šæ¨¡æ€æ¢ç´¢ã€‚

ä¾‹å¦‚ imgsï¼š 0001å¦™è›™ç§å­.png

è€ƒè™‘åˆ°clipå¯¹äºä¸­æ–‡æ”¯æŒä¸é‚£ä¹ˆä¼˜ç§€ã€‚æœ¬é¡¹ç›®åŸºäº [Chinese-CLIP ](https://github.com/OFA-Sys/Chinese-CLIP)ã€‚  

Chinese-CLIP æ˜¯CLIPæ¨¡å‹çš„**ä¸­æ–‡**ç‰ˆæœ¬ï¼Œä½¿ç”¨å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆ~2äº¿å›¾æ–‡å¯¹ï¼‰ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å¿«é€Ÿå®ç°ä¸­æ–‡é¢†åŸŸçš„[å›¾æ–‡ç‰¹å¾&ç›¸ä¼¼åº¦è®¡ç®—](https://github.com/OFA-Sys/Chinese-CLIP#APIå¿«é€Ÿä¸Šæ‰‹)ã€[è·¨æ¨¡æ€æ£€ç´¢](https://github.com/OFA-Sys/Chinese-CLIP#è·¨æ¨¡æ€æ£€ç´¢)ã€[é›¶æ ·æœ¬å›¾ç‰‡åˆ†ç±»](https://github.com/OFA-Sys/Chinese-CLIP#é›¶æ ·æœ¬å›¾åƒåˆ†ç±»)ç­‰ä»»åŠ¡ã€‚(å‚è€ƒChinese-Clip readme)



## å®‰è£…è¦æ±‚ ğŸ› 

å¼€å§‹æœ¬é¡¹ç›®å‰ï¼Œéœ€å…ˆæ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¸‹åˆ—ç¯å¢ƒé…ç½®è¦æ±‚:

- python >= 3.6.4
- pytorch >= 1.8.0 (with torchvision >= 0.9.0)
- CUDA Version >= 10.2

è¿è¡Œä¸‹åˆ—å‘½ä»¤å³å¯å®‰è£…æœ¬é¡¹ç›®æ‰€éœ€çš„ä¸‰æ–¹åº“ã€‚

```python
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹ ğŸš€

å®‰è£…Chinese-CLIPåï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°è°ƒç”¨APIï¼Œè¿›è¡Œå›¾æ–‡ç‰¹å¾æå–å’Œç›¸ä¼¼åº¦è®¡ç®—ã€‚é¦–å…ˆï¼Œå®‰è£…cn_clipï¼š

```bash
# é€šè¿‡pipå®‰è£…
pip install cn_clip

# æˆ–è€…ä»æºä»£ç å®‰è£…
cd Chinese-CLIP
pip install -e .
```

å®‰è£…æˆåŠŸåï¼Œå³å¯é€šè¿‡å¦‚ä¸‹æ–¹å¼è½»æ¾è°ƒç”¨APIï¼Œä¼ å…¥æŒ‡å®šå›¾ç‰‡ï¼ˆ[ç¤ºä¾‹](https://github.com/OFA-Sys/Chinese-CLIP/blob/master/examples/pokemon.jpeg)ï¼‰å’Œæ–‡æœ¬ï¼Œæå–å›¾æ–‡ç‰¹å¾å‘é‡å¹¶è®¡ç®—ç›¸ä¼¼åº¦ï¼š

```python
import torch 
from PIL import Image

import cn_clip.clip as clip
from cn_clip.clip import load_from_name, available_models
print("Available models:", available_models())  
# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = load_from_name("ViT-B-16", device=device, download_root='./')
model.eval()
image = preprocess(Image.open("examples/pokemon.jpeg")).unsqueeze(0).to(device)
text = clip.tokenize(["æ°å°¼é¾Ÿ", "å¦™è›™ç§å­", "å°ç«é¾™", "çš®å¡ä¸˜"]).to(device)

with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text)
    # å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œè¯·ä½¿ç”¨å½’ä¸€åŒ–åçš„å›¾æ–‡ç‰¹å¾ç”¨äºä¸‹æ¸¸ä»»åŠ¡
    image_features /= image_features.norm(dim=-1, keepdim=True) 
    text_features /= text_features.norm(dim=-1, keepdim=True)    

    logits_per_image, logits_per_text = model.get_similarity(image, text)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()

print("Label probs:", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]
```

æ³¨æ„è¿™é‡Œçš„pokemon.jpeg æ˜¯Chinese-clipç›®å½•ä¸­çš„ã€‚

## è®­ç»ƒæ‚¨è‡ªå·±çš„æ•°æ®é›† ğŸŒŸ

æœ¬é¡¹ç›®æ”¯æŒä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤æ¥ç»„ç»‡æ‚¨çš„ä»£ç å’Œæ•°æ®ï¼š

### ä»£ç ç»„ç»‡

ä¸‹è½½æœ¬é¡¹ç›®å, è¯·åˆ›å»ºæ–°çš„æ–‡ä»¶å¤¹ `${DATAPATH}` ä»¥å­˜æ”¾æ•°æ®é›†ã€é¢„è®­ç»ƒckptã€ä»¥åŠfinetuneäº§ç”Ÿçš„æ¨¡å‹æ—¥å¿—&ckptã€‚æ¨èå·¥ä½œåŒºç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```
Chinese-CLIP/
â”œâ”€â”€ run_scripts/
â”‚   â”œâ”€â”€ muge_finetune_vit-b-16_rbt-base.sh   #å®˜æ–¹æ˜¯å¤šå¡ï¼Œæˆ‘è¿™é‡Œä¿®æ”¹ä¸ºå•å¡
â”‚   â”œâ”€â”€ flickr30k_finetune_vit-b-16_rbt-base.sh
â”‚   â””â”€â”€ ...           # æ›´å¤šfinetuneæˆ–è¯„æµ‹è„šæœ¬...
â””â”€â”€ cn_clip/
    â”œâ”€â”€ clip/
    â”œâ”€â”€ eval/
    â”œâ”€â”€ preprocess/
    â””â”€â”€ training/

${DATAPATH}  #è¿™é‡Œæˆ‘æ”¾åœ¨ Chinese-CLIP/
â”œâ”€â”€ pretrained_weights/
â”œâ”€â”€ experiments/   #æ”¹ä¸º  ${DATAPATH}/logs/ å°±æ˜¯ Chinese-CLIP/logs
â”œâ”€â”€ deploy/	      # ç”¨äºå­˜æ”¾ONNX & TensorRTéƒ¨ç½²æ¨¡å‹
â””â”€â”€ datasets/   #  Chinese-CLIP/datasets
    â”œâ”€â”€ MUGE/     # 2G   
    â”œâ”€â”€ Flickr30k-CN/  2G å¤§å°
    â””â”€â”€ .../          # æ›´å¤šè‡ªå®šä¹‰æ•°æ®é›†...  è¿™é‡ŒåŠ å…¥pokemonæ•°æ®é›†
```



### åˆ¶ä½œè‡ªå·±çš„æ•°æ®é›†

æ‚¨å¯ä»¥åœ¨ `datasets` æ–‡ä»¶å¤¹ä¸‹æ‰¾åˆ° `pokemon` æ•°æ®é›†ã€‚ä½¿ç”¨ `data_processing` è„šæœ¬æ¥è°ƒæ•´æ‚¨çš„æ•°æ®æ ¼å¼ï¼š

- `data_t2s.py`: å¦‚æœæ‚¨çš„æ•°æ®æ˜¯ç¹ä½“ä¸­æ–‡ï¼Œä½¿ç”¨æ­¤è„šæœ¬è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡ã€‚
- `data_processing.py`: è´Ÿè´£åŠ è½½å¹¶å¤„ç†æ•°æ®ï¼Œæœ€ç»ˆç”Ÿæˆ `.tsv` å’Œ `.jsonl` æ–‡ä»¶ã€‚

### æ¨¡å‹å¾®è°ƒ

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼š

```bash
cd Chinese-CLIP/
bash run_scripts/muge_finetune_vit-b-16_rbt-base.sh ${DATAPATH}
```

è¯·æ ¹æ®æ‚¨çš„éœ€è¦è°ƒæ•´ `muge_finetune_vit-b-16_rbt-base.sh` è„šæœ¬ã€‚

### è®­ç»ƒç»†èŠ‚

-  ä½¿ç”¨3090 24 G batch_sizeè®¾ç½®ä¸º64ï¼Œ æ˜¾å­˜å æœ‰22G,åˆå§‹epochs  accç‰¹åˆ«ä½ 1% è¿™æ ·å­ï¼Œå¤šå°æœºå­éƒ½æ˜¯è¿™ä¸ªè¡¨ç°ã€‚ åœ¨epochs 414 accåˆ°è¾¾95%, æˆ‘æ–­æ‰äº†æœåŠ¡å™¨ã€‚ å‰414epochs : [epochs-best.pt](https://pan.baidu.com/s/15bw8LKaEuHX2u5XiIL6OEg?pwd=3032 )  æå–ç  ï¼š 3032 

-  V100  32 G   batch_sizeè®¾ç½®ä¸º 128   æ˜¾å­˜å æœ‰26Gï¼Œ åˆå§‹acc 83% ï¼Œè®­ç»ƒ30 epochs  text2imge å’Œ imge2textéƒ½åˆ°è¾¾ 98%ä½œç”¨ï¼Œ å½“epochs=50   æ—¶å€™text2imge å’Œ imge2text è¶…è¿‡99%  ï¼Œå‰ 50epochs :    [epochs-best.pt ](https://pan.baidu.com/s/14gP-eM7Pegg6quEpEpFgsw )  æå–ç ï¼šgogf 

 

## æ¨ç†ä¸è¯„ä¼° ğŸ•µï¸â€â™‚ï¸

ç”±äºChinese-clipå¹¶æ²¡æœ‰è¯´æ˜è‡ªå·±è®­ç»ƒæ•°æ®åæ¨ç†çš„ç»†èŠ‚ ï¼Œæ¥ä¸‹æ¥çš„æ¨ç†ç»†èŠ‚å¯ä»¥è®¤ä¸ºæ˜¯åŸé¡¹ç›®çš„è¡¥å……ã€‚

### 1.å®˜æ–¹ä»£ç æ¨ç†è‡ªå·±è®­ç»ƒé›†ä¸­çš„å›¾ç‰‡

```python


*import* torch 

*from* PIL *import* Image



*import* cn_clip.clip *as* clip

*from* cn_clip.clip *import* load_from_name, available_models

print("Available models:", available_models())  

*# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']*



device *=* "cuda" *if* torch.cuda.is_available() *else* "cpu"

model, preprocess *=* load_from_name("ViT-B-16", *device**=*device, *download_root**=*'./')

model.eval()

image *=* preprocess(Image.open("imgs/çš®å¡ä¸˜.png").convert("RGBA")).unsqueeze(0).to(device)  *#Label probs: [[0.003006 0.974   0.01017  0.01265 ]]*

*# image = preprocess(Image.open("imgs/çš®å¡ä¸˜.png")).unsqueeze(0).to(device)*

text *=* clip.tokenize(["æ°å°¼é¾Ÿ", "å¦™è›™ç§å­", "å°ç«é¾™", "çš®å¡ä¸˜"]).to(device)



*with* torch.no_grad():

  image_features *=* model.encode_image(image)

  text_features *=* model.encode_text(text)

  *# å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œè¯·ä½¿ç”¨å½’ä¸€åŒ–åçš„å›¾æ–‡ç‰¹å¾ç”¨äºä¸‹æ¸¸ä»»åŠ¡*

  image_features */=* image_features.norm(*dim**=-*1, *keepdim**=*True) 

  text_features */=* text_features.norm(*dim**=-*1, *keepdim**=*True)   



  logits_per_image, logits_per_text *=* model.get_similarity(image, text)

  probs *=* logits_per_image.softmax(*dim**=-*1).cpu().numpy()



print("Label probs:", probs)     # Label probs: [[0.002913 0.974    0.01017  0.01266 ]]  é¢„æµ‹ä¸ºå¦™è›™ç§å­  æ˜æ˜¾é”™è¯¯
```

 ` Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50'] Loading vision model config from /root/autodl-tmp/project/cn_clip/clip/model_configs/ViT-B-16.json Loading text model config from /root/autodl-tmp/project/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2} Label probs: [[0.002913 0.974    0.01017  0.01266 ]]` 



### 2. å¾®è°ƒåä½¿ç”¨epochs_best.ptæ¨ç†è‡ªå·±çš„æ•°æ®é›†

```python
 import torch
import torch.nn.functional as F
import cn_clip.clip as clip
from PIL import Image

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹æƒé‡
model_path = 'epoch_latest.pt'
saved_model = torch.load(model_path, map_location=torch.device('cpu'))
model_state_dict = saved_model['state_dict']

# è°ƒæ•´çŠ¶æ€å­—å…¸çš„é”®
adjusted_state_dict = {}
for key in model_state_dict.keys():
    new_key = key[7:] if key.startswith('module.') else key
    adjusted_state_dict[new_key] = model_state_dict[key]

# åˆ›å»ºæ¨¡å‹å®ä¾‹
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load_from_name("ViT-B-16", device=device)

# åŠ è½½è°ƒæ•´åçš„çŠ¶æ€å­—å…¸
try:
    model.load_state_dict(adjusted_state_dict)
except RuntimeError as e:
    print("Error:", e)
    model.load_state_dict(adjusted_state_dict, strict=False)

# è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()

# å›¾åƒé¢„å¤„ç†
image_path = "imgs/çš®å¡ä¸˜.png"
image = preprocess(Image.open(image_path).convert("RGBA")).unsqueeze(0).to(device)

# æ–‡æœ¬å¤„ç†
text = clip.tokenize(["æ°å°¼é¾Ÿ", "å¦™è›™ç§å­", "å°ç«é¾™", "çš®å¡ä¸˜"]).to(device)

# æ¨ç†
with torch.no_grad():
    image_features, text_features, logit_scale = model(image, text)

    # å½’ä¸€åŒ–ç‰¹å¾
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)

    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
    logits_per_image = logit_scale * image_features @ text_features.t()

    # è½¬æ¢ä¸ºæ¦‚ç‡
    probs_per_image = F.softmax(logits_per_image, dim=-1).cpu().numpy()

print("Label probabilities:", probs_per_image) #[[0.e+00 0.e+00 6.e-08 1.e+00]] é¢„æµ‹ä¸ºçš®å¡ä¸˜ ï¼

```

`Loading vision model config from /root/autodl-tmp/project/cn_clip/clip/model_configs/ViT-B-16.json Loading text model config from /root/autodl-tmp/project/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2} Label probabilities: [[0.e+00 0.e+00 6.e-08 1.e+00]]`

ä¸€ä¸ªæé†’ï¼š å½“ä½ è®­ç»ƒå®Œæ¨¡å‹è¿›è¡Œæ¨ç†æ—¶å€™ï¼Œè®­ç»ƒåptæƒé‡çš„é¡ºåºä¼šé”™ä¹± ï¼Œ**éœ€è¦åŠ è½½è°ƒæ•´åçš„çŠ¶æ€å­—å…¸** ã€‚

